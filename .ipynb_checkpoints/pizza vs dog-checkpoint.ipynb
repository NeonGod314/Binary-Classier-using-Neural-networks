{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @Shubham Kumar Singh\n",
    "\n",
    "## Digital enterprise, Advisian\n",
    "\n",
    "### Consits 2 models for binary classification b/w class0 and class1\n",
    "#### Model 1: 2 layered\n",
    "#### Model 2: 5 layered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'dnn_backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a8622d6fad6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdnn_backbone\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'dnn_backbone'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_backbone import *\n",
    "import glob\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "# don't touch_ sets parameters for plots_______________________________\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# _____________________________________________________________________\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading my data\n",
    "#train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "# Example of a picture\n",
    "# Explore your dataset \n",
    "#m_train = train_x_orig.shape[0]\n",
    "#num_px = train_x_orig.shape[1]\n",
    "#m_test = test_x_orig.shape[0]\n",
    "\n",
    "#print (\"Number of training examples: \" + str(m_train))\n",
    "#print (\"Number of testing examples: \" + str(m_test))\n",
    "#print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "#print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "#print (\"train_y shape: \" + str(train_y.shape))\n",
    "#print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "#print (\"test_y shape: \" + str(test_y.shape))\n",
    "# Reshape the training and test examples \n",
    "#train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "#test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "#train_x = train_x_flatten/255.\n",
    "#test_x = test_x_flatten/255.\n",
    "\n",
    "#print (\"train_x's shape: \" + str(train_x.shape))\n",
    "#print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset for TRAINING________________________________________________________\n",
    "\n",
    "file_pizza=glob.glob('class0/*.jpg')\n",
    "file_sunflower=glob.glob('class1/*.jpg')\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "\n",
    "#pizza\n",
    "for file in file_pizza:\n",
    "    img=np.array(mpimg.imread(file))\n",
    "    #print(img.shape)\n",
    "       \n",
    "    img_flatten = img.reshape(12288,1) \n",
    "    #print(img_flatten.shape)\n",
    "    standard_img=img_flatten / 255.\n",
    "    x_train.append(standard_img)\n",
    "    #print(standard_img)\n",
    "    y_train.append(0)\n",
    "\n",
    "    \n",
    "\n",
    "#sunflower\n",
    "for file in file_sunflower:\n",
    "    img=np.array(mpimg.imread(file))\n",
    "    #print(img.shape)\n",
    "       \n",
    "    img_flatten = img.reshape(12288,1) \n",
    "    #print(img_flatten.shape)\n",
    "    standard_img=img_flatten / 255.\n",
    "    x_train.append(standard_img)\n",
    "    #print(standard_img)\n",
    "    y_train.append(1)\n",
    "      \n",
    "x_train=np.array(x_train)\n",
    "y_train=np.array(y_train)\n",
    "\n",
    "x_train=np.squeeze(x_train)\n",
    "train_x=x_train.T\n",
    "train_y=y_train.reshape(1,82)\n",
    "\n",
    "print(\"Training: Shape of Input matrix  :\",train_x.shape)\n",
    "print(\"Training: Shape of Output matrix :\",train_y.shape)\n",
    "\n",
    "#loading dataset for TEST\n",
    "file_pizza_test=glob.glob('class0_test/*.jpg')\n",
    "file_sunflower_test=glob.glob('class1_test/*.jpg')\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "\n",
    "#pizza\n",
    "for file in file_pizza_test:\n",
    "    img_test=np.array(mpimg.imread(file))\n",
    "    #print(img.shape)\n",
    "       \n",
    "    img_flatten_test = img_test.reshape(12288,1) \n",
    "    #print(img_flatten.shape)\n",
    "    standard_img_test=img_flatten_test / 255.\n",
    "    x_test.append(standard_img_test)\n",
    "    #print(standard_img)\n",
    "    y_test.append(0)\n",
    "    \n",
    "\n",
    "#sunflower\n",
    "for file in file_sunflower_test:\n",
    "    img_test=np.array(mpimg.imread(file))\n",
    "    #print(img.shape)\n",
    "       \n",
    "    img_flatten_test = img_test.reshape(12288,1) \n",
    "    #print(img_flatten.shape)\n",
    "    standard_img_test=img_flatten_test / 255.\n",
    "    x_test.append(standard_img_test)\n",
    "    #print(standard_img)\n",
    "    y_test.append(1)\n",
    "      \n",
    "x_test=np.array(x_test)\n",
    "y_test=np.array(y_test)\n",
    "\n",
    "x_test=np.squeeze(x_test)\n",
    "test_x=x_test.T\n",
    "test_y=y_test.reshape(1,24)\n",
    "\n",
    "print(\"Training: Shape of Input matrix  :\",test_x.shape)\n",
    "print(\"Training: Shape of Output matrix :\",test_y.shape)\n",
    "print(y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input single test\n",
    "\n",
    "#loading dataset for TEST\n",
    "single_test=\"image_0001.jpg\"\n",
    "single_x=[]\n",
    "single_y=[]\n",
    "\n",
    "single_img=np.array(mpimg.imread(single_test))\n",
    "       \n",
    "single_img_flatten  = single_img.reshape(12288,1) \n",
    "print(single_img_flatten.shape)\n",
    "single_standard_img = single_img_flatten/ 255.\n",
    "single_x.append(single_standard_img)\n",
    "#print(single_standard_img)\n",
    "single_y.append(1) #input the class mannualy here\n",
    "\n",
    "\n",
    "single_x=np.array(single_x)\n",
    "single_y=np.array(single_y)\n",
    "print(single_y)\n",
    "\n",
    "single_x=np.squeeze(single_x)\n",
    "single_x=(single_x.T).reshape(12288,1)\n",
    "single_y=single_y.reshape(1,1)\n",
    "\n",
    "print(\"Training: Shape of Input matrix  :\",single_x.shape)\n",
    "print(\"Training: Shape of Output matrix :\",single_y.shape)\n",
    "print(single_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 12288     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_layer_model\n",
    "\n",
    "    \"\"\"\n",
    "    LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X              = input data, of shape (n_x, number of examples)\n",
    "    Y              = true \"label\" vector (containing 0 if class0, 1 if class1),[1, number of examples]\n",
    "    layers_dims    = dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations = number of iterations of the optimization loop\n",
    "    learning_rate  = learning rate of the gradient descent update rule\n",
    "    print_cost     = If set to True, this will print the cost every 100 iterations \n",
    "    Returns        = parameters :a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop for gradient descent\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation='relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation='sigmoid')\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation.Inputs=\"dA2, cache2, cache1\".Outputs:\"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation='sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation='relu')\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate=learning_rate)\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS: dimensios of the layer________________\n",
    "\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  5-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "    #[LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X = data, numpy array of shape [number of examples, num_px * num_px * 3]\n",
    "    Y = true \"label\" vector (containing 0 if class0, 1 if class1), [1, number of examples]\n",
    "    layers_dims    = list containing the input size and each layer size, of length number of layers + 1\n",
    "    learning_rate  = learning rate of the gradient descent update rule\n",
    "    num_iterations = number of iterations of the optimization loop\n",
    "    print_cost     = if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters     = parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate=learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_pred_test = predict(single_x, single_y, parameters)\n",
    "\n",
    "plt.imshow(np.array(ndimage.imread(single_test, flatten=False)))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
